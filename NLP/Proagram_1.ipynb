{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def tokenize_word(text):\n",
    "    words = [word for word in word_tokenize(text) if not is_stop_word(word)]\n",
    "    return words\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def is_stop_word(word):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    return word.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_tf(word,sentence):\n",
    "    words=tokenize_word(sentence)\n",
    "    return words.count(word)/len(words)\n",
    "\n",
    "def calculate_idf(word,sentences):\n",
    "    no=sum(1 for sentence in sentences if word in tokenize_word(sentence))\n",
    "    return math.log(len(sentences)/(no+1))\n",
    "\n",
    "def calculate_tf_idf(sentence,sentences):\n",
    "    words=set(tokenize_word(sentence))\n",
    "    tf_idf_scores=0\n",
    "    for word in words:\n",
    "        tf=calculate_tf(word,sentence)\n",
    "        idf=calculate_idf(word,sentences)\n",
    "        tf_idf_scores+=tf*idf\n",
    "    return tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sentence(scores):\n",
    "    max_score=float('-inf')\n",
    "    max_sentence=None\n",
    "    for sentence,score in scores.items():\n",
    "        if(score>max_score):\n",
    "            max_score=score\n",
    "            max_sentence=sentence\n",
    "    return max_sentence\n",
    "\n",
    "def n_largest(scores,n):\n",
    "    sentences=[]\n",
    "    for i in range(n):\n",
    "        max_sentence=find_max_sentence(scores)\n",
    "        sentences.append(max_sentence)\n",
    "        del scores[max_sentence]\n",
    "    return sentences\n",
    "\n",
    "def summarize_text(text,length):\n",
    "    sentences=tokenize_sentence(text)\n",
    "    sentence_scores={sentence:calculate_tf_idf(sentence,sentences) for sentence in sentences}\n",
    "    selected_sentences=n_largest(sentence_scores,length)\n",
    "    summary=' '.join(selected_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Already in 1940, Alan Turing published an article titled Computing Machinery and Intelligence which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.Natural language processing has its roots in the 1940s. Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics.It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them.\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics.It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.Natural language processing has its roots in the 1940s.[1] Already in 1940, Alan Turing published an article titled Computing Machinery and Intelligence which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence\"\n",
    "summary = summarize_text(text,5)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
