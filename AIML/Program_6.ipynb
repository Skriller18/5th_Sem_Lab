{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score\n",
    "def read_data(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def __init__(self):\n",
    "        self.class_prob={}\n",
    "        self.features_prob={}\n",
    "    def fit(self,X_train,Y_train):\n",
    "        classes,counts=np.unique(Y_train,return_counts=True)\n",
    "        total_samples=len(Y_train)\n",
    "        for c,count in zip(classes,counts):\n",
    "            self.class_prob[c]=count/total_samples\n",
    "        self.features_prob={}\n",
    "        for c in classes:\n",
    "            self.features_prob[c]={}\n",
    "            for feature in X_train.columns:\n",
    "                unique_values=X_train[feature].unique()\n",
    "                self.features_prob[c][feature]={}\n",
    "                for value in unique_values:\n",
    "                    count = np.sum((X_train[feature] == value) & (Y_train == c))\n",
    "                    self.features_prob[c][feature][value] = count / counts[c]\n",
    "                   \n",
    "                   \n",
    "    def predict(self,X_test):\n",
    "        predictions=[]\n",
    "        for _,row in X_test.iterrows():\n",
    "            max_prob=-1\n",
    "            predicted_class=None\n",
    "            for c in self.class_prob:\n",
    "                prob = self.class_prob[c]\n",
    "                for feature, value in row.items():\n",
    "                    if value in self.features_prob[c][feature]:\n",
    "                        prob *= self.features_prob[c][feature][value]\n",
    "                    else:\n",
    "                        prob *= 0\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    predicted_class = c\n",
    "                predictions.append(predicted_class)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"Social_Network_Ads.csv\")\n",
    "data['Gender'] = data['Gender'].apply(lambda x: 1 if x == \"Male\" else 0)\n",
    "X = data.iloc[:,1:4]\n",
    "y = data['Purchased']\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NaiveBayes()\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(len(Y_pred)):\n",
    "    if i % 2 != 0:\n",
    "        y_pred.append(Y_pred[i])\n",
    "Y_test = Y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics:\n",
      "Accuracy: 0.85\n",
      "Precision: 0.73\n",
      "Recall: 0.73\n",
      "F1 Score: 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_pred, Y_test)\n",
    "precision = precision_score(y_pred, Y_test)\n",
    "recall = recall_score(y_pred, Y_test)\n",
    "f1 = f1_score(y_pred, Y_test)\n",
    "\n",
    "print(\"Validation Set Metrics:\")\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52  6]\n",
      " [ 6 16]]\n",
      "Class 0 predicted and true : \n",
      "52\n",
      "Class 0 predicted and false : \n",
      "6\n",
      "Class 1 predicted and false : \n",
      "6\n",
      "Class 1 predicted and true : \n",
      "16\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_pred,Y_test)\n",
    "print(confusion)\n",
    "print(\"Class 0 predicted and true : \")\n",
    "print(confusion[0][0])\n",
    "print(\"Class 0 predicted and false : \")\n",
    "print(confusion[0][1])\n",
    "print(\"Class 1 predicted and false : \")\n",
    "print(confusion[1][0])\n",
    "print(\"Class 1 predicted and true : \")\n",
    "print(confusion[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = data.sample(n=20)\n",
    "X_valid = valid.iloc[:,1:4]\n",
    "y_valid = valid['Purchased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = model.predict(X_valid)\n",
    "y_valpred = []\n",
    "for i in range(len(y_val)):\n",
    "    if i % 2 != 0:\n",
    "        y_valpred.append(y_val[i])\n",
    "y_valid = y_valid.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics:\n",
      "Accuracy: 0.90\n",
      "Precision: 0.88\n",
      "Recall: 0.88\n",
      "F1 Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_valpred,y_valid)\n",
    "precision = precision_score(y_valpred,y_valid)\n",
    "recall = recall_score(y_valpred,y_valid)\n",
    "f1 = f1_score(y_valpred,y_valid)\n",
    "\n",
    "print(\"Validation Set Metrics:\")\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  1]\n",
      " [ 1  7]]\n",
      "Class 0 predicted and true : \n",
      "11\n",
      "Class 0 predicted and false : \n",
      "1\n",
      "Class 1 predicted and false : \n",
      "1\n",
      "Class 1 predicted and true : \n",
      "7\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_valpred,y_valid)\n",
    "print(confusion)\n",
    "print(\"Class 0 predicted and true : \")\n",
    "print(confusion[0][0])\n",
    "print(\"Class 0 predicted and false : \")\n",
    "print(confusion[0][1])\n",
    "print(\"Class 1 predicted and false : \")\n",
    "print(confusion[1][0])\n",
    "print(\"Class 1 predicted and true : \")\n",
    "print(confusion[1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
